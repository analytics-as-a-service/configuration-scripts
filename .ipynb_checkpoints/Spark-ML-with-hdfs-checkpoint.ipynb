{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16003665",
   "metadata": {},
   "source": [
    "# Spark-ML-with-hdfs\n",
    "- Preprocessing, machine learning, and integration with hadoop is done in this notebook\n",
    "- I have written a script that automatically runs whenever a jupyter notebook is launched(script is provided [here](https://github.com/analytics-as-a-service/configuration-scripts/tree/main/jupyterhub-spark)). This script gives the spark context and spark session in `sc` and `spark` variables respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df24213-3e4f-4f21-b800-cfd3634c41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session, Pipeline, Functions, and Metrics\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Keras / Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Elephas for Deep Learning on Spark\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888459b-73b4-431f-9e81-c4eadca29702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b5ff3-f8fa-4636-a815-326fe681fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261bf18-4002-4831-af0f-feef498af363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType,IntegerType,StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e046c3c-f231-40b2-b843-e12633bf8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in environ.keys():\n",
    "    temp = i.find(\"HDFS_NN_PORT\")\n",
    "    if temp>=0 and len(i)-temp==12 and environ[i].find(\"9000\")>=0:\n",
    "        hadoop=environ[i].replace(\"tcp\",\"hdfs\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa68a11-893c-47b8-afd7-4ccce703c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = spark.read.format('com.databricks.spark.csv').options(header=\"true\",inferschema='true').load(f'{hadoop}/dataset/train_transaction.csv')\n",
    "train_identity =  spark.read.format('com.databricks.spark.csv').options(header=\"true\",inferschema='true').load(f'{hadoop}/dataset/train_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721df2b5-4fca-4788-978c-651c23ba8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = train_transaction.join(train_identity,\"TransactionID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9f90aa-a74f-43f7-8b48-f8c8b2e1f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9576979-3bf3-4f51-a249-8f017df790e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pd.read_csv('./imp_features.csv',index_col='Unnamed: 0')\n",
    "cols = cols.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519b9d6-2052-4d93-8ec2-d38dac817b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_d.select(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c87538-12a5-41da-a509-8c7fef900993",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var = [ i[\"name\"] for i in train.schema.jsonValue()[\"fields\"] if i[\"type\"]==\"string\" ]\n",
    "num_var = [ i[\"name\"] for i in train.schema.jsonValue()[\"fields\"] if i[\"type\"]==\"integer\" or i[\"type\"]==\"double\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ed602-7800-44fa-a827-2ee9dd92ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_to_scale(df=train, feature_list=[], lower_skew=-2, upper_skew=2):\n",
    "    selected_features = []\n",
    "    temp = df.select(*feature_list).toPandas()\n",
    "    for feature in feature_list:\n",
    "        if temp[feature].kurtosis() < -2 or temp[feature].kurtosis() > 2:\n",
    "            selected_features.append(feature)\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6136aa-6de0-4dcb-ade1-5b1d3694a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"isFraud\" in cat_var:\n",
    "    cat_var.remove(\"isFraud\")\n",
    "if \"isFraud\" in num_var:\n",
    "    num_var.remove(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257c446-4762-4efb-8149-c67acbb30bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=cat_var, outputCols=[i + \"_index\" for i in cat_var]).setHandleInvalid(\"keep\")\n",
    "imputer = Imputer(inputCols=string_indexer.getOutputCols(),outputCols=[i + \"_imputed\" for i in cat_var]).setStrategy(\"mode\")\n",
    "encoder = OneHotEncoder(inputCols=imputer.getOutputCols(),outputCols=[i + \"_class_vec\" for i in cat_var])\n",
    "stages += [string_indexer,imputer, encoder]\n",
    "\n",
    "imputer = Imputer(inputCols=num_var,outputCols=[i+\"_imputed\" for i in num_var]).setStrategy(\"mean\")\n",
    "stages += [imputer]\n",
    "\n",
    "unscaled_features = select_features_to_scale(df=train, feature_list=num_var)\n",
    "unscaled_assembler = VectorAssembler(inputCols=[i+\"_imputed\" for i in unscaled_features], outputCol=\"unscaled_features\")\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"scaled_features\")\n",
    "stages += [unscaled_assembler, scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b78a9-1723-4edf-963d-0888cb95c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_str_indexer =  StringIndexer(inputCol=\"isFraud\", outputCol=\"Y\")\n",
    "num_unscaled_diff_list = list(set(num_var) - set(unscaled_features))\n",
    "assembler_inputs = [i + \"_class_vec\" for i in cat_var] + [i+\"_imputed\" for i in num_unscaled_diff_list] + [\"scaled_features\"]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"X\")\n",
    "stages += [label_str_indexer, assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c766e-71af-4e4f-a53b-cd045e1a63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe8ec8-71ba-423c-96bd-811be81c8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pipeline_model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef171e-721f-4cb2-a646-1c487d7c31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train = df_train.select(\"X\",\"Y\").toDF(\"features\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b66f6-7a17-4e9a-9dfb-92c381e27367",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98427167-c545-4ca8-a05b-47c43ba75801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "\n",
    "vectorized=_train\n",
    "k = 2\n",
    "minorityClass = 1\n",
    "majorityClass = 0\n",
    "percentageOver = 200\n",
    "percentageUnder = 100\n",
    "if(percentageUnder > 100|percentageUnder < 10):\n",
    "    raise ValueError(\"Percentage Under must be in range 10 - 100\");\n",
    "if(percentageOver < 100):\n",
    "    raise ValueError(\"Percentage Over must be in at least 100\");min_Array[i][0]\n",
    "dataInput_min = vectorized[vectorized['label'] == minorityClass]\n",
    "dataInput_maj = vectorized[vectorized['label'] == majorityClass]\n",
    "\")\n",
    "ratio = dataInput_maj.count()/dataInput_min.count()\n",
    "\n",
    "a = range(round(ratio))\n",
    "oversampled_minority_df = dataInput_min.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "oversampled_df = dataInput_maj.unionAll(oversampled_minority_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6437913-e930-423c-9350-5fc97490b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae4cd1-6469-4752-92e0-cbe34d199b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10)\n",
    "lrt = lr.fit(oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafe0c1-52fa-4ccb-ba26-53acfcbab649",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.save(f\"{hadoop}/pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61327a-968b-4c58-85f0-2b1eb9b87d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrt.save(f\"{hadoop}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894f05a-3f94-491e-aca0-9b4dd215bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1bc231-52b1-475f-b909-6baa68e4181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predvsact = lrt.transfddorm(_train).select(\"label\",\"prediction\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbae3c1-2b13-4cc5-9495-2834f6e88443",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predvsact[\"label\"],predvsact[\"prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49665b-1884-4796-aa7b-c39d100709e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rft = rf.fit(oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ab682-dea0-446b-80f5-0f22d23c9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predvsact = lrt.transform(_train).select(\"label\",\"prediction\").toPandas()\n",
    "print(classification_report(predvsact[\"label\"],predvsact[\"prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532aaf53-a411-4e7a-a47f-f2e073e28775",
   "metadata": {},
   "outputs": [],
   "source": [
    "predvsact = rft.transform(_train).select(\"label\",\"prediction\").toPandas()\n",
    "print(classification_report(predvsact[\"label\"],predvsact[\"prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3685fd84-8126-469d-869a-a56d1b4b0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_classes = _train.select(\"label\").distinct().count()\n",
    "# input_dim = len(_train.select(\"features\").first()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4beb2e4-2bc8-4318-a06d-095ab6a325ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(10, input_shape=(input_dim,), activity_regularizer=regularizers.l2(0.01)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(rate=0.3))\n",
    "# model.add(Dense(nb_classes))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3872d-0475-4227-861f-828912160c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_conf = optimizers.Adam(learning_rate=0.01)\n",
    "# opt_conf = optimizers.serialize(optimizer_conf)\n",
    "\n",
    "# estimator = ElephasEstimator()\n",
    "# estimator.setFeaturesCol(\"features\")\n",
    "# estimator.setLabelCol(\"label\")\n",
    "# estimator.set_keras_model_config(model.to_json())\n",
    "# estimator.set_categorical_labels(True)\n",
    "# estimator.set_nb_classes(nb_classes)\n",
    "# estimator.set_num_workers(1)\n",
    "# estimator.set_epochs(2) \n",
    "# estimator.set_batch_size(64)\n",
    "# estimator.set_verbosity(1)\n",
    "# estimator.set_validation_split(0.10)\n",
    "# estimator.set_optimizer_config(opt_conf)\n",
    "# estimator.set_mode(\"synchronous\")\n",
    "# estimator.set_loss(\"binary_crossentropy\")\n",
    "# estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987cc44c-56f6-4d96-bc56-63f5a2bb3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e7b0a-d7c1-41d1-96be-7bd21173787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlt = dl_pipeline.fit(_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596032d-62a0-400c-b9d9-b057c1aba4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5e4f3-0eac-4cc1-a5fb-918939d145b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# from functools import reduce\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.sql.functions import rand,col,when,concat,substring,lit,udf,lower,sum as ps_sum,count as ps_count,row_number\n",
    "# from pyspark.sql.window import *\n",
    "# from pyspark.sql import DataFr`ame\n",
    "# from pyspark.ml.feature import VectorAssembler,BucketedRandomProjectionLSH,VectorSlicer\n",
    "# from pyspark.sql.window import Window\n",
    "# from pyspark.ml.linalg import Vectors,VectorUDT,SparseVector\n",
    "# from pyspark.sql.functions import array, create_map, struct\n",
    "\n",
    "# def subudf(arr):\n",
    "#     # Must decorate func as udf to ensure that its callback form is the arg to df iterator construct\n",
    "#     a = arr[0]\n",
    "#     b = arr[1]\n",
    "#     if isinstance(a, SparseVector):\n",
    "#         a = a.toArray()\n",
    "#     if isinstance(b, SparseVector):\n",
    "#         b = b.toArray()\n",
    "#     array_ = a - b\n",
    "#     return random.uniform(0, 1) * Vectors.dense(array_)\n",
    "# subtract_vector_udf = udf(subudf)\n",
    "\n",
    "# def addudf(arr):\n",
    "#     # Must decorate func as udf to ensure that its callback form is the arg to df iterator construct\n",
    "#     a = arr[0]\n",
    "#     b = arr[1]\n",
    "#     if isinstance(a, SparseVector):\n",
    "#         a = a.toArray()\n",
    "#     if isinstance(b, SparseVector):\n",
    "#         b = b.toArray()\n",
    "#     array_ = a + b\n",
    "#     return Vectors.dense(array_)\n",
    "# add_vector_udf = udf(subudf)\n",
    "\n",
    "# def smote(vectorized_sdf, seed, k, bucketLength, multiplier):\n",
    "#     dataInput_min = vectorized_sdf[vectorized_sdf['label'] == 1]\n",
    "#     dataInput_maj = vectorized_sdf[vectorized_sdf['label'] == 0]\n",
    "    \n",
    "#     # LSH, bucketed random projection\n",
    "#     brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",seed=seed, bucketLength=bucketLength)\n",
    "#     # smote only applies on existing minority instances    \n",
    "#     model = brp.fit(dataInput_min)\n",
    "#     model.transform(dataInput_min)\n",
    "\n",
    "#     # here distance is calculated from brp's param inputCol\n",
    "#     self_join_w_distance = model.approxSimilarityJoin(dataInput_min, dataInput_min, float(\"inf\"), distCol=\"EuclideanDistance\")\n",
    "\n",
    "#     # remove self-comparison (distance 0)\n",
    "#     self_join_w_distance = self_join_w_distance.filter(self_join_w_distance.EuclideanDistance > 0)\n",
    "\n",
    "#     over_original_rows = Window.partitionBy(\"datasetA\").orderBy(\"EuclideanDistance\")\n",
    "\n",
    "#     self_similarity_df = self_join_w_distance.withColumn(\"r_num\", row_number().over(over_original_rows))\n",
    "\n",
    "#     self_similarity_df_selected = self_similarity_df.filter(self_similarity_df.r_num <= k)\n",
    "\n",
    "#     over_original_rows_no_order = Window.partitionBy('datasetA')\n",
    "\n",
    "#     # list to store batches of synthetic data\n",
    "#     res = []\n",
    "    \n",
    "#     # two udf for vector add and subtract, subtraction include a random factor [0,1]\n",
    "#     subtract_vector_udf = udf(lambda arr: random.uniform(0, 1)*(subtract_vector_udf(arr)), VectorUDT())\n",
    "#     add_vector_udf = udf(lambda arr: add_vector_udf(arr), VectorUDT())\n",
    "    \n",
    "#     # retain original columns\n",
    "#     original_cols = dataInput_min.columns\n",
    "    \n",
    "#     for i in range(multiplier):\n",
    "#         print(\"generating batch %s of synthetic instances\"%i)\n",
    "#         # logic to randomly select neighbour: pick the largest random number generated row as the neighbour\n",
    "#         df_random_sel = self_similarity_df_selected.withColumn(\"rand\", F.rand()).withColumn('max_rand', F.max('rand').over(over_original_rows_no_order))\\\n",
    "#                             .where(F.col('rand') == F.col('max_rand')).drop(*['max_rand','rand','r_num'])\n",
    "#         # create synthetic feature numerical part\n",
    "#         df_vec_diff = df_random_sel.select('*', subtract_vector_udf(array('datasetA.features', 'datasetB.features')).alias('vec_diff'))\n",
    "#         df_vec_modified = df_vec_diff.select('*', add_vector_udf(array('datasetA.features', 'vec_diff')).alias('features'))\n",
    "        \n",
    "#         # for categorical cols, either pick original or the neighbour's cat values\n",
    "#         for c in original_cols:\n",
    "#             # randomly select neighbour or original data\n",
    "#             col_sub = random.choice(['datasetA','datasetB'])\n",
    "#             val = \"{0}.{1}\".format(col_sub,c)\n",
    "#             if c != 'features':\n",
    "#                 # do not unpack original numerical features\n",
    "#                 df_vec_modified = df_vec_modified.withColumn(c,col(val))\n",
    "        \n",
    "#         # this df_vec_modified is the synthetic minority instances,\n",
    "#         df_vec_modified = df_vec_modified.drop(*['datasetA','datasetB','vec_diff','EuclideanDistance'])\n",
    "        \n",
    "#         res.append(df_vec_modified)\n",
    "    \n",
    "#     dfunion = reduce(DataFrame.unionAll, res)\n",
    "#     # union synthetic instances with original full (both minority and majority) df\n",
    "#     oversampled_df = dfunion.union(vectorized_sdf.select(dfunion.columns))\n",
    "    \n",
    "#     return oversampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458ba09-f4cf-4b75-b6e5-fcd932abeec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampled_df = smote(_train, 32, 2, 10, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252669d-0b23-494d-9f4a-fb10c584be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf5f18-2c61-47e8-93bc-f6dc6617c5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
